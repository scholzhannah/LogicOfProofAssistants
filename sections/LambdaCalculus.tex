\section{\texorpdfstring{$\lambda$}{lambda}-calculus}

\begin{boxdefi}
    Let $\alert{\fV} = \{ x_0, x_1, \dots \} $ a countably infinite set of variables and $\alert{C} = \{ c_0, c_1, \dots \}$ be any set of constants. 
    The terms of the \alert{$\lambda$-calculus} are given by the following BNF: 
    \begin{equation*}
        s,t \Coloneqq x \mid c \mid (s t) \mid (\lambda x. t)
    \end{equation*}
    where $x$ is a variable and $c$ a constant.
\end{boxdefi}

\begin{rem}
    \hfill
    \begin{enumerate}
        \item Think of $(\lambda x. t)$ as the function $x \mapsto t(x)$ and $(s t)$ as function application. 
        \item Anything can apply to any term, e.g. $(x x)$ is a term.
        \item Abbreviate $((rs)t)u$ to $rstu$ and $\lambda x. \lambda y. \lambda z. t$ to $\lambda x y z. t$. For example, $\lambda xy. xy$ means $(\lambda x. (\lambda y. (x y)))$.
        \item $\lambda x.t$ binds the variable $x$. Like in first-order logic we can define $\alpha$-equivalence ($\aeq$) and substitution ($t[s/x]$). Here we identify $\alpha$-equivalent terms, e.g. $\lambda x.x = \lambda y.y$.
    \end{enumerate}
\end{rem}

\begin{example}
    $(x(\lambda x.x))[s/x] = s(\lambda x.x)$
\end{example}

\begin{rem}
    Consider $(\lambda x. t) s$. 
    We want this to correspond to $t[s/x]$. 
\end{rem}

\begin{boxdefi}
    \hfill
    \begin{enumerate}
        \item \alert{$\beta$-contraction ($\becont$)} is defined as $(\lambda x. t) s \becont t [s/x]$. $(\lambda x. t) s$ is called a \alert{$\beta$-redex} and $t [s/x]$ is called its \alert{$\beta$-reduct}.
        \item {\alert{One-step-$\beta$-reduction ($\beored$)} is defined as the compatible closure if $\becont$, i.e.
            \begin{enumerate}
                \item If $s \becont t$ then $s \beored t$.
                \item If $s \beored t$ then $su \beored tu$, $us \beored ut$ and $\lambda x. s \beored \lambda x. t$.
            \end{enumerate}}
        \item \alert{$\beta$-reduction ($\bered$)} is the reflexive transitive closure of $\beored$, i.e. it is the smallest relation that is reflexive, transitive and contains $\beored$.
        \item \alert{$\beta$-equivalence ($\beq$)} is the smallest equivalence relation containing $\bered$.
    \end{enumerate}
\end{boxdefi}

\begin{example}\label{ex:betared}
    \hfill
    \begin{enumerate}
        \item $(\lambda x. xxy)(yz) \becont yz(yz)y$
        \item \adjustbox{valign=t}{
            \begin{tikzcd}
                (\lambda x. xx)y((\lambda z. yz)(ww))  \ar[r, "{\beta , 1}"] \ar[d, "{\beta , 1}"] & (\lambda x.xx)y(y(ww)) \ar[d, "{\beta , 1}"] \\
                yy ((\lambda z. yz)(ww)) \ar[r, "{\beta , 1}"] & yy (y(ww)) \\
            \end{tikzcd}}
            \vspace{-2em}
        \item $(\lambda x.xx) (\lambda x. xx) \beored (\lambda x.xx) (\lambda x. xx)$
    \end{enumerate}
\end{example}

\begin{boxdefi}
    We define the following combinators: 
    \begin{enumerate}
        \item $\alert{I} = \lambda x.x$
        \item $\alert{K} = \lambda xy.x$
        \item $\alert{K_*} = \lambda xy.y$
        \item $\alert{S} = \lambda xyz.xz(yz)$
    \end{enumerate}
\end{boxdefi}

\begin{rem}
    Every term can be defined using $K$ and $S$ up to $\beta$-equivalence.
\end{rem}

\begin{example}
    $SKK \bered \lambda z. Kz(Kz) \bered \lambda z.z = I$.
\end{example}

\begin{boxprop} \label{prop:fixpoi}
    There exists a \alert{fixed-point combinator} $Y$ such that $Y t \bered t(Yt)$.
\end{boxprop}
\begin{proof}
    Let $A \coloneq \lambda fx. x(ffx)$ and let $Y \coloneq AA$ be the \alert{Turing operator}.
    Then $Yt = AAt \bered t (AAt) = t(Yt)$.
\end{proof}

\begin{boxdefi}
    We can define the \alert{pairing} $\alert{P} \coloneq \lambda stx.xst$ and denote $\alert{(s, t)} \coloneq Pst \bered \lambda x. xst$.
\end{boxdefi}

\begin{rem}
    Naming this a pairing makes sense because we have $(s, t) K \bered Kst \bered s$ and $(s, t) K_* \bered K*(s, t) \bered t$.
\end{rem}

\begin{boxdefi}
    We can define \alert{Church numerals}. 
    If $n$ is a natural number we encode it as $\alert{[n]} \coloneq \lambda fx. f^n x$ where $f^0 x \coloneq x$ and $f^{n+1} x = f(f^nx) = f^n(fx)$.
\end{boxdefi}

\begin{boxdefi}
    Let $A$ be a set and $\ored$ a binary relation on $A$ with reflective transitive closure $\to$.
    \begin{enumerate}
        \item $t \in A$ is \alert{in normal form} if there is no $s$ such that $t \ored s$.
        \item $t \in A$ \alert{has normal form $s$} if $s$ is in normal form and $t \to s$.
        \item $t \in A$ is \alert{strongly normalizing} if there exists no infinite sequence $t \ored t_1 \ored t_2 \ored t_3 \ored \cdots$.
        \item $\ored$ is \alert{(weakly) normalizing} if every $t \in A$ has a normal form.
        \item $\ored$ is \alert{strongly normalizing} if every $t \in A$ is strongly normalizing.
        \item {$\ored$ is \alert{confluent} (has the \alert{Church-Rosser property}) if whenever $u \leftarrow t \to v$ there is an $s \in A$ with $u \to s \leftarrow v$.
            \begin{equation*}
            \begin{tikzcd}[ampersand replacement=\&]
                t \ar[r] \ar[d] \& v \ar[d, dashed] \\
                u \ar[r, dashed] \& s 
            \end{tikzcd}
        \end{equation*}}
    \end{enumerate}
\end{boxdefi}

\begin{rem}
    $\beta$-reduction is neither weakly nor strongly normalizing.
\end{rem}
\begin{proof}
    See the counterexamples presented in Example \ref{ex:betared} (iii) and Proposition \ref{prop:fixpoi}.
\end{proof}

\begin{boxthm}[Church-Rosser] \label{thm:ChurchRosser}
    $\beored$ is confluent.
\end{boxthm}

\begin{rem}
    It does not suffice to prove 
    \begin{equation*}
        \begin{tikzcd}
            t \ar[r, "{\beta, 1}"] \ar[d, "{\beta, 1}"] & v \ar[d, dashed, "\beta"] \\
            u \ar[r, dashed, "\beta"] & s 
        \end{tikzcd}
    \end{equation*}
    i.e. for a general binary relation Theorem \ref{thm:ChurchRosser} does not follow from this.
\end{rem}

\begin{boxdefi} \label{def:parared}
    \alert{Parallel reduction ($\Rightarrow$)} is defined inductively as
    \begin{enumerate}
        \item $x \Rightarrow x$ and $c \Rightarrow c$ where $x$ is a variable and $c$ is a constant.
        \item If $t \Rightarrow t'$ then $\lambda x.t \Rightarrow \lambda x. t'$.
    \end{enumerate}
    If $t \Rightarrow t'$ and $u \Rightarrow u'$ then 
    \begin{enumerate}[resume]
        \item $tu \Rightarrow t'u'$.
        \item $(\lambda x.t)u \Rightarrow t'[u'/x]$.
    \end{enumerate}
\end{boxdefi}

\begin{boxlem}
    Parallel induction is reflexive, i.e. $t \Rightarrow t$.
\end{boxlem}
\begin{proof}
    We show this by induction on $t$. 
    Consider $t = \lambda x.s$. 
    Then by induction hypothesis $s \Rightarrow s$, so by Definition \ref{def:parared} (ii) $\lambda x.s \Rightarrow \lambda x.s$.
    The rest of the cases are similar.
\end{proof}

\begin{boxlem} \label{lem:beoredtoarrow}
    If $t \beored s$ then $t \Rightarrow s$.
\end{boxlem}
\begin{proof}
    We show this by induction on $t \beored s$.
    If $t \becont s$, say $(\lambda x.u)v \becont u [v/x]$. 
    Then $(\lambda x. u)v \Rightarrow u [v/x]$ by Definition \ref{def:parared} (iv).
    The other cases are also easy to show.
\end{proof}

\begin{boxlem} \label{lem:arrowbered}
    If $t \Rightarrow t'$ then $t \bered t'$.
\end{boxlem}
\begin{proof}
    We show this by induction on $t \Rightarrow t'$.
    Suppose the last rule was Definition \ref{def:parared} (iv), i.e. concluding $(\lambda x.t)u \Rightarrow t'[u'/x]$ from $t \Rightarrow t'$ and $u \Rightarrow u'$.
    By induction hypothesis we have $t \bered t'$ and $u \bered u'$.
    Then $(\lambda x.t)u \bered (\lambda x.t') u \bered (\lambda x. t') u' \beored t'[u'/x]$.
    The other cases are similar.
\end{proof}

\begin{boxlem} \label{lem:arrowsubst}
    If $t \Rightarrow t'$ and $w \Rightarrow w'$ then $t[w/y] \Rightarrow t'[w'/y]$.
\end{boxlem}
\begin{proof}
    We show this by induction on $t \Rightarrow t'$.
    Suppose the last step was Definition \ref{def:parared} (iv), i.e. concluding $(\lambda x.t)u \Rightarrow t'[u'/x]$ from $t \Rightarrow t'$ and $u \Rightarrow u'$.
    By induction hypothesis we know that $t[w/y] \Rightarrow t'[w/y]$ and $u[w /y] \Rightarrow u'[w/y]$.
    We have to show $(\lambda x.t [w/y])u[w/y] \Rightarrow t'[u'/x][w'/y]$.
    $x$ is bound so we may assume that $x$ is not free in $w'$. 
    Then one can prove that $t'[u'/x][w'/y] = t'[w'/y][(u'[w'/y])/x]$.
    Now the claim follows from Definition \ref{def:parared} (iv).
    The rest of the cases are similar.
\end{proof}

\begin{boxdefi} \label{def:star}
    If $t$ is a term then \alert{$t^*$} is recursively defined as 
    \begin{enumerate}
        \item $x^* = x$ and $c^* = c$ for variables $x$ and constants $c$.
        \item $(\lambda x.t)^* = \lambda x. t^*$.
        \item $(ts)^* = t^*s^*$ if $t$ is not a $\lambda$.
        \item $((\lambda x.t) s)^* = t^*[s^*/x]$.
    \end{enumerate}
\end{boxdefi}

\begin{boxlem} \label{lem:arrowstar}
    If $s \Rightarrow t$ then $t \Rightarrow s^*$.
\end{boxlem}
\begin{proof}
    We show this by induction of the length of the derivation for $s \Rightarrow t$.

    If the last step was Definition \ref{def:parared} (iii), i.e. concluding $tu \Rightarrow t'u'$ from $t \Rightarrow t'$ and $u \Rightarrow u'$, then by induction hypothesis $t' \Rightarrow t^*$ and $u' \Rightarrow u^*$.
    We need to show that $t'u' \Rightarrow (t u)^*$.
    If $t$ is not a $\lambda$ then $(tu)^* = t^*u^*$, so we are done by Definition \ref{def:parared} (iii).
    If $t = \lambda x. s$ then $(tu)^* = s^* [u*/x]$.
    We know $\lambda x.s \Rightarrow t'$ which can only be derived using Definition \ref{def:parared} (ii). 
    So $t' = \lambda x. s'$ with $s \Rightarrow s'$.
    By induction hypothesis $s' \Rightarrow s^*$. 
    Then $(\lambda x. s') u' \Rightarrow s^* [u*/x]$ follows from Definition \ref{def:star} (iv).

    If the last step was Definition \ref{def:parared} (iv), i.e. concluding $(\lambda x.t) u \Rightarrow t'[u'/x]$ from $t \Rightarrow t'$ and $u \Rightarrow u'$, then by induction hypothesis we know that $t' \Rightarrow t^*$ and $u' \Rightarrow u^*$.
    Then $t'[u'/x] \Rightarrow ((\lambda x.t)u)^* = t^*[u^* /x]$ by Lemma \ref{lem:arrowsubst}.

    The rest of the cases are easy.
\end{proof}

\begin{boxlem}\label{lem:almostconfluence}
    If $t \beored u$ and $t \bered v$ then there is an $s$ with $u \bered s \prescript{}{\beta}{\leftarrow} v$.
    \begin{equation*}
        \begin{tikzcd}[ampersand replacement=\&]
            t \ar[r, "\beta"] \ar[d, "{\beta, 1}"] \& v \ar[d, dashed, "\beta"] \\
            u \ar[r, dashed, "\beta"] \& s 
        \end{tikzcd}
    \end{equation*}
\end{boxlem}
\begin{proof}
    Decomposing $t \bered v$ into individual steps gives us
    \begin{equation*}
        \begin{tikzcd}[/tikz/cells={/tikz/nodes={shape=asymmetrical
            rectangle,text width=0.5cm,text height=2ex,text depth=0.3ex,align=center}}]
            & t \ar[dl, "{\beta, 1}"] \ar[dr, "{\beta, 1}"] & \\
            u && v_1 \ar[dr, "{\beta, 1}"] \\
            &&& v_2 \ar[dr, "{\beta, 1}"] \\
            &&&& \ddots \ar[dr, "{\beta, 1}"] \\
            &&&&& \mathllap{v_n} = \mathrlap{v}
        \end{tikzcd}
    \end{equation*}
    which with Lemma \ref{lem:beoredtoarrow} becomes
    \begin{equation*}
        \begin{tikzcd}[/tikz/cells={/tikz/nodes={shape=asymmetrical
            rectangle,text width=0.5cm,text height=2ex,text depth=0.3ex,align=center}}]
            & t \ar[dl, Rightarrow] \ar[dr, Rightarrow] & \\
            u && v_1 \ar[dr, Rightarrow] \\
            &&& v_2 \ar[dr, Rightarrow] \\
            &&&& \ddots \ar[dr, Rightarrow] \\
            &&&&& \mathllap{v_n} = \mathrlap{v}
        \end{tikzcd}
    \end{equation*}
    which with Lemma \ref{lem:arrowstar} yields
    \begin{equation*}
        \begin{tikzcd}[/tikz/cells={/tikz/nodes={shape=asymmetrical
            rectangle,text width=0.5cm,text height=2ex,text depth=0.3ex,align=center}}]
            & t \ar[dl, Rightarrow] \ar[dr, Rightarrow] & \\
            u \ar[dr, Rightarrow] && v_1 \ar[dr, Rightarrow] \ar[dl, Rightarrow] \\
            & t^* \ar[dr, Rightarrow] && v_2 \ar[dr, Rightarrow] \ar[dl, Rightarrow] \\
            && v_1^* \ar[dr, Rightarrow] && \ddots \ar[dr, Rightarrow] \\
            &&& \ddots \ar[dr, Rightarrow] && \mathllap{v_n} = \mathrlap{v} \ar[dl, Rightarrow]\\
            &&&&\mathllap{v}_{n\mathrlap{-1}}^*
        \end{tikzcd}
    \end{equation*}
    which implies our desired statement since by Lemma \ref{lem:arrowbered} parallel reduction implies $\beta$-reduction and $\beta$-reduction is transitive. 
\end{proof}

\begin{exercise}
    Derive Theorem \ref{thm:ChurchRosser} (Church-Rosser) from Lemma \ref{lem:almostconfluence}.
\end{exercise}

\begin{boxcor}
    Every term $t$ has at most one $\beta$-normal form.
\end{boxcor}

\begin{boxcor}
    If $s \beq t$, then there is $u$ such that $s \bered u \prescript{}{\beta}{\leftarrow} t$.
\end{boxcor}

\subsection{Partial recursive functions}

\begin{boxdefi}
    Let $\alert{A \rightharpoonup B}$ be the set of \alert{partial functions} from $A$ to $B$, i.e. a partial function from $A$ to $B$ is a pair $(X, f)$ such that $X \subseteq A$ and $f \colon X \to B$.
    We set $\alert{\domain{f}} \coloneq X$ and call it the \alert{domain} of $f$.
\end{boxdefi}

\begin{notation}
    We set $\alert{f(\vec{x})\down} \coloneq \vec{x} \in \domain{f}$ and $\alert{f(\vec{x})\up} \coloneq \vec{x} \notin \domain{f}$.
\end{notation}

\begin{boxdefi}\label{def:pr}
    The set of \alert{partial recursive functions (P.R. functions)} is the subset of $\bigcup_{k \ge 0}(\bN^k \rightharpoonup \bN)$ generated by
    \begin{enumerate}
        \item $0 \colon \bN^0 \to \bN$ is P.R.
        \item $\text{Succ} \colon \bN \to \bN, x \mapsto x + 1$ is P.R.
        \item The projection $\proj{i}{k} \colon \bN^k \to \bN,  \proj{i}{k}(x_0, \dots, x_{k-1}) = x_i$ is P.R. for $0 \le i < k$.
        \item Composition: if $f \colon \bN^k \to \bN$ is P.R. and $g_0, \dots, g_{k-1} \colon \bN^l \to \bN$ are P.R. then $f \circ \vec{g} \colon \bN^l \to \bN, x \mapsto f(g_0(\vec{x}), \dots, g_{k-1}(\vec{x}))$ is P.R. and $\vec{x} \in \domain{f \circ \vec{g}}$ iff $\vec{x} \in \domain{g_i}$ for all $i$ and $(g_0(\vec{x}), \dots, g_{k-1}(\vec{x})) \in \domain{f}$.
    \item Primitive recursion: if $f_0 \colon \bN^k \to \bN$ and $f_s \colon \bN^{k+2} \to \bN$ are P.R. then the function $g$ defined by $g(0, \vec{x}) \coloneq f_0(\vec{x})$ and $g(n+1, \vec{x}) \coloneq f_s(n, g(n, \vec{x}), \vec{x})$ is P.R., $g(0, \vec{x})\down$ iff $f_0(\vec{x})\down$, and $g(n + 1, \vec{x})\down$ iff $g(n, \vec{x})\down$ and $f_s(n, g(n, \vec{x}), \vec{x})\down$. 
    \item Minimization (unbounded search): if $f \colon \bN^{k + 1} \to \bN$ is P.R. then $\mu_f \colon \bN^k \to \bN$ is P.R. where we set $\mu_f(\vec{x}) = n$ iff $f(i, \vec{x})\down$ for $i \le n$, $f(i, \vec{x}) > 0$ for $i < n$ and $f(n, \vec{x}) = 0$, and we set $\mu_f(\vec{x})\up$ if no such $n$ exists.
    \end{enumerate}
\end{boxdefi}

\begin{rem}
    \hfill
    \begin{enumerate}
        \item The class of functions closed under the rules (i) to (iv) of Definition \ref{def:pr} are called the \alert{primitive recursive functions}. All  of them are total.
        \item The partially recursive functions that are total are called \alert{(totally) recursive}. Not all of them are primitive recursive functions.
        \item The Church-Turing thesis is the claim that the partially recursive functions are precisely the functions that capture the intuitive notion of ``computability''.
    \end{enumerate}
\end{rem}

\begin{example}
    \hfill
    \begin{enumerate}
        \item $(x, y) \mapsto x + y$, $(x, y) \mapsto x \cdot y$ and $(x, y) \mapsto x^y$ are all primitive recursive.
        \item The \alert{Ackermann function} is a total partial recursive function that is not primitive recursive.
    \end{enumerate}
\end{example}

\begin{boxdefi}
    A $\lambda$-term $t$ \alert{respresents} $f \colon \bN^k \to \bN$ if for all $\vec{n} \in \bN^k$ we have $t[n_0] \dots [n_{k-1}] \bered [f(\vec{n})]$ if $f(\vec{n})\down$ and that $t[n_0] \dots [n_{k-1}]$ has no normal form if $f(\vec{n})\up$.
\end{boxdefi}

\begin{boxprop}
    If $f \colon \bN^k \to \bN$ is P.R. then there are primitive recursive functions $u \colon \bN \to \bN$ and $t \colon \bN^{k+ 1} \to \bN$ such that $f = u \circ \mu_t$.
\end{boxprop}

\begin{boxthm}
    $f \colon \bN^k \to \bN$ is P.R. iff it is represented by some $\lambda$-term.
\end{boxthm}
\begin{proof}[Proof sketch]
    For the backward direction of the proof first notice that primitive recursive functions are expressive enough to encode: pairs of natural numbers, finite sequences of natural numbers, $\lambda$-terms, substitution in $\lambda$-terms and $\beta$-reduction.
Then we can show that there is a partial recursive function that searches for a reduction sequence $t \beored t_1 \beored t_2 \beored \dots \beored [m]$.

For the forward direction we first show that the claim is true for a primitive recursive function $f$.
We now proceed inductively over the first five constructors of Definition \ref{def:pr}. 
For (i) we see that $[0]$ represents $0 \colon \bN^0 \to \bN$.
Constructor (ii) is an exercise. 
For (iii) we can show that the projection $\proj{i}{k}$ is represented by $\lambda x_0 x_1 \dots x_{k-1}. x_i$.
For (iv) assume that $F$ represents $f$ and $G_i$ represents $g_i$ for all $i$. 
Then $f \circ \vec{g}$ is represented by $\lambda \vec{x}. F (G_0 \vec{x}) (G_1 \vec{x}) \dots (G_{k-1} \vec{x})$.
Lastly, for (v) suppose that $F_0$ represents $f_0$ and $F_s$ represents $f_s$. 
We want to find $G$ such that 
\begin{align*}
    G[0]\vec{x} &\beq F_0 \vec{x} & G[n + 1]\vec{x} &\beq F_s [n] (G[n] \vec{x})\vec{x} \\
    \intertext{This is sufficient because $G[0][\vec{n}] \beq F_0[\vec{n}] \bered [f_0(\vec{n})]$ so $G[0][\vec{n}] \bered [f_0(\vec{n})]$ since $[f_0(\vec{n})]$ is in normal form. It is even enough to show that }
    G[0] &\beq F_0 & G[n + 1] &\beq F_s[n](G[n])
\end{align*}
by $\lambda$-abstraction.
We want to try to encode the sequence $([0], G[0]), ([1], G[1]), \dots$
We set 
\begin{equation*}
T \coloneq \lambda u. (\suc{uK}, F_s(uK)(uK_*))
\end{equation*}
Note that $T([n], t) \bered (\suc{[n]}, F_s[n]t) \bered ([n + 1], F_s[n]t)$.
We will show later that means that $T$ produces the next pair in the sequence. 
So we want to iterate $T$.
Let 
\begin{equation*}
    G \coloneq \lambda v.vT([0], F_0)K_*
\end{equation*}
Then 
\begin{equation*}
    G[0] \bered [0]T([0], F_0)K_* \bered ([0], F_0) K_* \bered F_0
\end{equation*}

We want to show by induction that $[n]T([0], F_0) \beq ([n], G[n])$.
For the case $n = 0$ we get $[0]T([0], F_0) \bered ([0], F_0) \prescript{}{\beta}{\leftarrow} ([0], G[0])$.
Thus $[0]T([0], F_0) \beq ([0], G[0])$.
For $n + 1$ we get
\begin{equation*}
    [n + 1]T([0], F_0) \bered T([n]T([0], F_0)) \stackrel{IH}{\beq} T([n], G[n]) \bered ([n + 1], F_s [n](G[n]))
\end{equation*}
and 
\begin{equation*}
    G[n + 1] \bered [n + 1]T([0], F_0)K_*  \beq ([n + 1], F_s [n](G[n]))K_* \bered F_s[n](G[n])
\end{equation*}
Thus $[n + 1]T([0], F_0) \beq ([n + 1], G[n + 1])$ and consequently $G[n + 1] \beq F_s [n](G[n])$.

\hfill

Now onto the general case.
For minimization we use the Turing operator $Y$ from the proof of Proposition \ref{prop:fixpoi} and the term $D$ with $Dst[0] \bered s$ and $Dst[n + 1] \bered t$.
(The existence of $D$ is an exercise.) 
Let $T$ be a term and $\vec{x}$ be a sequence of variables. 
We define 
\begin{equation*}
    W \coloneq Y (\lambda vy. Dy (v (\suc{y}))(Ty \vec{x}))
\end{equation*}
Then we get 
\begin{equation*}
    W[n] \bered D[n](W[n + 1])(T[n]\vec{x}) \bered 
    \begin{cases}
        [n] & \text{if } T[n]\vec{x} \bered [0] \\
        W[n + 1] & \text{if } T[n] \vec{x} \bered [m + 1] \text{ for some } m \in \bN
    \end{cases}
\end{equation*}
By the previous proposition we can take primitive recursive functions $u$ and $t$ such that $f = u \circ \mu_t$. 
Suppose that $U$ represents $u$ and $T$ represents $t$.
Now we set 
\begin{equation*}
    F \coloneq \lambda \vec{x}. U (W[0])
\end{equation*}
If $f(\vec{x})\down$ then 
\begin{equation*}
    F[\vec{x}] \bered U(W[0]) \bered U([\mu_t(\vec{x})]) \bered [f(\vec{x})]
\end{equation*}
and if $f(\vec{x})\up$ then 
\begin{equation*}
    F[\vec{x}] \bered U(W[0]) \bered U(W[1]) \bered \dots
\end{equation*}
which is an infinite reduction sequence. 
We would have to show that no way of reduction leads to a normal form. 
This is true but we omit the proof.
\end{proof}

\begin{boxthm}[Normalization theorem]
    If $t$ has a $\beta$-normal form, then iterated contraction of the leftmost $\beta$-redex leads to its normal form. 
\end{boxthm}